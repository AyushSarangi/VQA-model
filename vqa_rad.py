# -*- coding: utf-8 -*-
"""VQA-RAD(Inter IIT ML - Team 7).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11IzrqA0ybi4bJGe6Kc3a9N3ksLg0Zl2d

## **Medical Image Visual-Question Answering(MED-VQA) on VQA-RAD Dataset**

### **Importing and Installing necessary dependencies**
"""

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

pip install transformers

#Importing necessary libraries and modules.
import os
import pandas as pd
import numpy as np
from PIL import Image

#Dependencies corresponding to the Image Encoder portion.
import tensorflow as tf
from tensorflow.keras.applications import VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Concatenate, Input, Dense, Lambda, Dropout, Attention, BatchNormalization
from tensorflow.image import resize
from tensorflow.keras.preprocessing import image
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score

#Dependencies corresponding to the Question Encoder portion.
import torch
from transformers import BertTokenizer, BertModel
model_name = "dmis-lab/biobert-v1.1"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

#Path where all the images are stored.
src_path="/content/drive/MyDrive/VQA_RAD Image Folder"

"""### **Image Encoder Model**"""

#Defining our base VGG19 Model.
base_model = VGG19(weights='imagenet', include_top=False)

# Define intermediate layers from which to extract features
intermediate_layers = [base_model.get_layer('block1_pool').output,
                        base_model.get_layer('block2_pool').output,
                        base_model.get_layer('block3_pool').output]

resized_features = []

for layer in intermediate_layers:
    resized = Lambda(lambda x: resize(x, (16, 16)))(layer)
    resized_features.append(resized)

#Concatenate the intermediate layers, that have been resized to the same shape to implement the concatenation.
concatenated_features = Concatenate()(resized_features)

#We now make use of an attention layer to implement a self attention mechanism.
attention = Attention(use_scale=True)([concatenated_features, concatenated_features])

#We use the weights to generate weightes feature maps.
weighted_features = tf.keras.layers.Multiply()([concatenated_features, attention])

# Flatten the weighted features.
output_layer = Flatten()(weighted_features)

#Define a new model, that will accept the images in our dataset as the input, and generate encodings of them.
new_model = Model(inputs=base_model.input, outputs=output_layer)

new_model.summary()

# Image preprocessing to ensure the inputs provided to the VGG-based model are in the proper format.
def preprocess_image(image_path):
    img = image.load_img(image_path, target_size=(224, 224))
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = preprocess_input(img)
    return img

image_encodings = {}

# Generate image encodings for each image in the folder by combining the feature maps generated at various depths in the VGG Convnet.
for filename in os.listdir(src_path):
    if filename.endswith(".jpg"):
        image_path = os.path.join(src_path, filename)
        img = preprocess_image(image_path)
        encoding = new_model.predict(img)
        image_encodings[filename]=encoding

"""### **Question Encoder Model and Dataset Preprocessing**"""

def text_preprocess(question):
    # Convert to lowercase
    text = str(question).lower()

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenize the text
    tokens = word_tokenize(text)

    # Remove stop words
    stop_words = set(stopwords.words('english'))
    stop_words.remove('no') #'no' is a stopword but at the same time also an answer, so we cannot remove it from our dataset.
    filtered_tokens = [word for word in tokens if word not in stop_words]

    # Lemmatize words
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]

    # Join the tokens back into a clean text
    clean_text = " ".join(lemmatized_tokens)

    return(clean_text)

data = pd.read_excel("/content/drive/MyDrive/VQA_RAD Dataset Public.xlsx")
data.head()

#Applying text preprocessing to the 'QUESTION' and 'ANSWER' columns of our dataset.
data['QUESTION'] = data['QUESTION'].apply(text_preprocess)
data['ANSWER'] = data['ANSWER'].apply(text_preprocess)

#Question encoding function using the BioBERT transformer.
def encode_question_answer_pairs(row):
    encoding = tokenizer(row['QUESTION'], row['ANSWER'], return_tensors='pt', padding=True, truncation=True)
    return encoding

data['encoded'] = data.apply(encode_question_answer_pairs, axis=1)

#Generating the question encodings.
def extract_features(row):
    with torch.no_grad():
        outputs = model(**row['encoded'])
        features = outputs.last_hidden_state.mean(dim=1).squeeze().tolist()
    return features

# Apply the feature extraction function, and generating a new column 'ques_feat' that contains the feature mappings of each question.
data['ques_feat'] = data.apply(extract_features, axis=1)

# The 'features' column now contains the extracted features
print(data['ques_feat'])

#Generating the image id's for each image, in order to map each image name to its image encoding in the image_encodings dictionary.
k=data.shape[0]
data['image_name']=''
for i in range(k):
  data['image_name']=data['IMAGEID'].str.split('/').str[-1]

#Creation of a new column 'image_feat' under which for each image the image encodings are present.
k=data.shape[0]
data['image_feat']=''
for i in range(k):
  data['image_feat'][i]=image_encodings[data['image_name'][i]]

#Pre-processing the values in the 'ques_feat' column that contains the question encodings, to convert them into a form suitable for input into our fusion model.
k=data.shape[0]
for i in range(k):
  data['ques_feat'][i]=np.array(data['ques_feat'][i])
  data['ques_feat'][i]=data['ques_feat'][i].reshape(1,-1)

"""### **Fusion Model**"""

#We generate the question encodings and image encodings from the dataset that are to be used for
question_encodings = np.array(data['ques_feat'].tolist())
image_encodings = np.array(data['image_feat'].tolist())

#We apply the np.squeeze() function to convert the shapes of the question_encodings and image_encodings to a 2D array of features - (None,q_dim) and(None,i_dim)
#Here, q_dim and i_dim are the number of values in the encodings of each question and image in the question-image pair respectively.
question_encodings = np.squeeze(question_encodings, axis=1)
image_encodings = np.squeeze(image_encodings, axis=1)

question_shape = question_encodings.shape[1] #q_dim
image_shape = image_encodings.shape[1]#i_dim
#The number of different answers in the 'ANSWERS' column of the dataset(will serve as the number of classes predicted by our model).
num_classes = len(pd.unique(data['ANSWER']))

#Fusion Model.

question_input = Input(shape=(question_shape,))
image_input = Input(shape=(image_shape,))

# Define individual fully connected layers for question and image modalities
question_fc = Dense(512, activation='relu')(question_input)
image_fc = Dense(512, activation='relu')(image_input)

# Apply attention mechanism to capture the interaction between modalities
attention = Attention(use_scale=True)([question_fc, image_fc])

# Weighted sum of the image features based on attention
weighted_image = Concatenate()([image_fc, attention])

# Now we add Dense layers to the fusion model.
fusion_layer = Dense(512, activation='relu')(weighted_image)
fusion_layer = Dense(512, activation='relu')(fusion_layer)

# Add batch normalization
fusion_layer = BatchNormalization()(fusion_layer)

# Add dropout to mitigate overfitting
fusion_layer = Dropout(0.5)(fusion_layer)

# Based on the same patter, we add more dense layers along with dropout layers as well as Batch Normalization.
fusion_layer = Dense(256, activation='relu')(fusion_layer)
fusion_layer = BatchNormalization()(fusion_layer)
fusion_layer = Dropout(0.5)(fusion_layer)

fusion_layer = Dense(128, activation='relu')(fusion_layer)
fusion_layer = BatchNormalization()(fusion_layer)
fusion_layer = Dropout(0.5)(fusion_layer)

# Output layer that returns the predicted probabilities for each answer.
output = Dense(num_classes, activation='softmax')(fusion_layer)

model = Model(inputs=[question_input, image_input], outputs=output)

#Compilation of the model using the 'adam'(adaptive moment estimation) optimizer, and the categorical cross entropy loss function.
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

#Target class.
ans=np.array(data['ANSWER'].tolist())

le=LabelEncoder()
ans=le.fit_transform(ans)

#Defining the training and testing datasets.
X_train, X_test, question_train, question_test, image_train, image_test, y_train, y_test = train_test_split(
    data,question_encodings, image_encodings, ans, test_size=0.2, random_state=42)

#Applying One Hot Encoding on the y_train values to make them suitable for the training process.
y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=484)

#Fusion Model training.
model.fit([question_train, image_train], y_train_one_hot, epochs=20, batch_size=32)

"""### **Model Performance**"""

predictions = model.predict([question_test, image_test])

y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=484)

#Accuracy.
true_labels = np.argmax(y_test_one_hot, axis=1)
predicted_labels = np.argmax(predictions, axis=1)

accuracy = accuracy_score(true_labels, predicted_labels)

print("Accuracy:", accuracy*100)

#F1 Score.
f1=f1_score(predicted_labels,true_labels,average='weighted')
print("F1 Score :",f1)





# Feature Extraction From images using ViT_MAE (visual Transformers + Masked auto enconders). Can be used in Future to increse accuracy and predict better results.
# This code is not the part of main code.

!pip install transformers
from transformers import AutoImageProcessor, ViTMAEModel
from PIL import Image
import requests

import numpy as np
import os
import h5py
from tensorflow.keras.preprocessing.image import load_img, img_to_array

image_processor = AutoImageProcessor.from_pretrained("facebook/vit-mae-base")
model = ViTMAEModel.from_pretrained("facebook/vit-mae-base")
dataLen = len(os.listdir('/content/drive/MyDrive/VQA_RAD Image Folder'))
dataLen

def extract_feat(image_folder, batch_size=16):
    img_feat = []
    dataLen = len(os.listdir(image_folder))
    filenames = sorted(os.listdir(image_folder))
    image_processor = AutoImageProcessor.from_pretrained("facebook/vit-mae-base")
    model = ViTMAEModel.from_pretrained("facebook/vit-mae-base")

    for start_idx in range(0,316, batch_size):
        end_idx = min(start_idx + batch_size, dataLen)
        batch_filenames = filenames[start_idx:end_idx]
        batch_images = [load_img(os.path.join(image_folder, fname)) for fname in batch_filenames]

        inputs = image_processor(images=batch_images, return_tensors="pt")
        outputs = model(**inputs)
        feat = outputs.last_hidden_state
        img_feat.append(feat)

    return img_feat

img_feat = extract_feat('/content/drive/MyDrive/VQA_RAD Image Folder')
x = []
for i in range(4):
  for j in range(len(out[i])):
    arr = out[i][j].detach().numpy()
    x.append(arr)

file_path = "features_316.npy"
np.save(file_path, x)

from google.colab import files
files.download(file_path)